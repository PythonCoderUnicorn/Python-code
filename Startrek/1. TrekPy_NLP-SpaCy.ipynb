{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural Language Processing with **SpaCy**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## spaCy - what is it?\n",
    "\n",
    "NLP is part of Machine Learning (ML) amd spaCy is a Python library that is open source which can handle and understand large volumes of text data (it can handle various languages too, except for Klingon, Bajoran and other Federation languages). \n",
    "\n",
    "## Install & setup\n",
    "- for detailed instructions:  https://spacy.io/usage/\n",
    "- step 1: **Terminal**: <code> pip3 install -U spacy</code> or <code> conda install -c conda-forge spacy</code>\n",
    "- step 2: as admin/sudo <code> python -m spacy download en </code>\n",
    "<br>\n",
    "- successful when you see:\n",
    "\n",
    "\n",
    "     Linking successful\n",
    "      // your computer path of download\n",
    "    You can now load the model via spacy.load('en')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Import spaCy** in notebook or Python\n",
    "\n",
    "- this takes awhile, as spaCy has a fairly large library to load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Captain\n",
      ": PROPN\n",
      "compound \n",
      "\n",
      "Sisko\n",
      ": PROPN\n",
      "nsubj \n",
      "\n",
      "loves\n",
      ": VERB\n",
      "ROOT \n",
      "\n",
      "Bajor\n",
      ": NOUN\n",
      "dobj \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Import spaCy and load the language library\n",
    "import spacy\n",
    "\n",
    "# ---- load spacy\n",
    "# spacy has en_core_web_ (small: sm, medium: md and large: lg) \n",
    "nlp = spacy.load('en_core_web_sm') \n",
    "\n",
    "#---- create a Document object\n",
    "# some documentation will have nlp(u\"string example\")\n",
    "# but the u in the (u\"s...\") is optional, stands for unicode\n",
    "doc = nlp(\"Captain Sisko loves Bajor\")\n",
    "\n",
    "# each word in the string is a token\n",
    "# a for-loop to print out English word structures of our doc\n",
    "# spaCy has token method functions for words in the doc\n",
    "for token in doc:\n",
    "    print(token.text) \n",
    "    print(':',token.pos_) # part of speech\n",
    "    print(token.dep_,'\\n') # syntax dependency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quark\n",
      ": PROPN\n",
      "nsubj \n",
      "\n",
      "is\n",
      ": AUX\n",
      "aux \n",
      "\n",
      "making\n",
      ": VERB\n",
      "ROOT \n",
      "\n",
      "45\n",
      ": NUM\n",
      "nummod \n",
      "\n",
      "bars\n",
      ": NOUN\n",
      "dobj \n",
      "\n",
      "of\n",
      ": ADP\n",
      "prep \n",
      "\n",
      "Latinum\n",
      ": PROPN\n",
      "pobj \n",
      "\n",
      "or\n",
      ": CCONJ\n",
      "cc \n",
      "\n",
      "$\n",
      ": SYM\n",
      "quantmod \n",
      "\n",
      "2.3\n",
      ": NUM\n",
      "compound \n",
      "\n",
      "million\n",
      ": NUM\n",
      "conj \n",
      "\n",
      "each\n",
      ": DET\n",
      "det \n",
      "\n",
      "year\n",
      ": NOUN\n",
      "npadvmod \n",
      "\n"
     ]
    }
   ],
   "source": [
    "doc2 = nlp(u\"Quark is making 45 bars of Latinum or $2.3 million each year\") \n",
    "\n",
    "for token in doc2:\n",
    "    print(token.text) \n",
    "    print(':',token.pos_) # part of speech\n",
    "    print(token.dep_,'\\n') # syntax dependency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "if you are wondering what PROPN, AUX, CCONJ etc mean, spaCy can explain it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'proper noun'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spacy.explain('PROPN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'coordinating conjunction'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spacy.explain('CCONJ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokens are step one of the **NLP Pipeline** process\n",
    "![](Pipeline.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part of Speech (POS) Tagging\n",
    "- https://spacy.io/api/annotation/#pos-tagging \n",
    "\n",
    "\n",
    "- doc = nlp(u\"Quark is making 45 bars of Latinum or $2.3 million each year\")\n",
    "\n",
    "\n",
    "|Tag|Description|doc2[0].tag|\n",
    "|:------|:------:|:------|\n",
    "|`.text`|The original word text<!-- .element: style=\"text-align:left;\" -->|`Quark`|\n",
    "|`.lemma_`|The base form of the word|`quark`|\n",
    "|`.pos_`|The simple part-of-speech tag|`PROPN`/`proper noun`|\n",
    "|`.tag_`|The detailed part-of-speech tag|`NNP`/`noun, proper singular`|\n",
    "|`.shape_`|The word shape – capitalization, punctuation, digits|`Xxxxx`|\n",
    "|`.is_alpha`|Is the token an alpha character?|`True`|\n",
    "|`.is_stop`|Is the token part of a stop list, i.e. the most common words of the language?|`False`|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tagger', 'parser', 'ner']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## NLP pipeline\n",
    "#  NER = name entity recognizer \n",
    "\n",
    "nlp.pipe_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Odo : PROPN\n",
      "is : AUX\n",
      "n't : PART\n",
      "investigating : VERB\n",
      "Quark : PROPN\n",
      "for : ADP\n",
      "the : DET\n",
      "first : ADJ\n",
      "time : NOUN\n",
      ", : PUNCT\n",
      "in : ADP\n",
      "awhile : ADJ\n"
     ]
    }
   ],
   "source": [
    "doc3 = nlp(u\"Odo isn't investigating Quark for the first time, in awhile\")\n",
    "\n",
    "for token in doc3:\n",
    "    print(token.text,':',token.pos_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Odo\n",
      "Odo\n"
     ]
    }
   ],
   "source": [
    "# lemmas\n",
    "print(doc3[0].text)\n",
    "print(doc3[0].lemma_)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROPN\n",
      "NNP / noun, proper singular\n"
     ]
    }
   ],
   "source": [
    "# Simple Parts-of-Speech & Detailed Tags:\n",
    "print(doc3[4].pos_)\n",
    "print(doc3[4].tag_ + ' / ' + spacy.explain(doc3[4].tag_)) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Odo: Xxx\n",
      "Bajor : Xxxxx\n"
     ]
    }
   ],
   "source": [
    "# Word Shapes: (word size) \n",
    "print(doc3[0].text +': '+ doc3[0].shape_)\n",
    "print(doc[3].text +' : '+ doc[3].shape_) \n",
    "# Odo = 3 letters = word size 3 = xxx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".is_alpha : True\n",
      ".is_stop : False\n"
     ]
    }
   ],
   "source": [
    "# Boolean Values:\n",
    "print('.is_alpha :',doc2[0].is_alpha)\n",
    "print('.is_stop :',doc2[0].is_stop) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Span**\n",
    "\n",
    "- span is a slice of a large document in order to easily work with\n",
    "- <code> doc[start : stop] </code>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "TNG= nlp(u\"Space: the final frontier. These are the voyages of the starship Enterprise.\\\n",
    "Its five-year mission: to explore strange new worlds. To seek out new life and new civilizations. \\\n",
    "To boldly go where no [humanoid] has gone before!\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "five-year mission: to explore strange new worlds. To seek out\n"
     ]
    }
   ],
   "source": [
    "trek_quote = TNG[16:30] \n",
    "print(trek_quote) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "spacy.tokens.span.Span"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(trek_quote) # our TNG doc got sliced or span"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "if you want to print out just the sentences "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Space: the final frontier.\n",
      "These are the voyages of the starship Enterprise.\n",
      "Its five-year mission: to explore strange new worlds.\n",
      "To seek out new life and new civilizations.\n",
      "To boldly go where no [humanoid] has gone before!\n"
     ]
    }
   ],
   "source": [
    "for sent in TNG.sents:\n",
    "    print(sent) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## get your **Tokenization** done here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization\n",
    "-  **Prefix**:\tCharacter(s) at the beginning &#9656; `$ ( “ ¿`\n",
    "-  **Suffix**:\tCharacter(s) at the end &#9656; `km ) , . ! ”`\n",
    "-  **Infix**:\tCharacter(s) in between &#9656; `- -- / ...`\n",
    "-  **Exception**: Special-case rule to split a string into several tokens or prevent a token from being split when punctuation rules are applied &#9656; `St. U.S.`\n",
    "\n",
    "\n",
    "### Prefixes, Suffixes and Infixes\n",
    "\n",
    "spaCy will isolate punctuation that does *not* form an integral part of a word. \n",
    "- Quotation marks, commas, and punctuation at the end of a sentence will be assigned their own token. \n",
    "- However, punctuation that exists as part of an email address, website or numerical value will be kept as part of the token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"We're moving to Vulcan!\"\n",
      "\" | We | 're | moving | to | Vulcan | ! | \" | "
     ]
    }
   ],
   "source": [
    "# Create a string that includes \n",
    "# opening and closing quotation marks\n",
    "\n",
    "mystring = '\"We\\'re moving to Vulcan!\"'\n",
    "print(mystring)\n",
    "\n",
    "\n",
    "# Create a Doc object and explore tokens\n",
    "doc = nlp(mystring)\n",
    "\n",
    "for token in doc:\n",
    "    print(token.text, end=' | ') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You\n",
      "have\n",
      "reached\n",
      "the\n",
      "Starfleet\n",
      "IT\n",
      "Department\n",
      ",\n",
      "if\n",
      "you\n",
      "are\n",
      "experiencing\n",
      "problems\n",
      ",\n",
      "send\n",
      "a\n",
      "message\n",
      "to\n",
      "our\n",
      "comlink\n",
      ",\n",
      "com\n",
      "-\n",
      "channel\n",
      "extension\n",
      "is\n",
      "6433\n",
      ",\n",
      "or\n",
      "visit\n",
      "https://askpython.com\n",
      "for\n",
      "further\n",
      "help\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "doc4 = nlp(\"You have reached the Starfleet IT Department,\\\n",
    "if you are experiencing problems, send a message to our comlink,\\\n",
    "com-channel extension is 6433, or visit https://askpython.com for further help.\") \n",
    "\n",
    "for t in doc4:\n",
    "    print(t) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "notice that the link remains together and clickable. spaCy is pretty cool."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "35"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# how many tokens are there in doc4?  \n",
    "# use len() function\n",
    "len(doc4) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "569"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# how many Vocab word objects in spaCy library?\n",
    "len(nlp.vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### get **token** by index position and slice\n",
    "\n",
    "- tokens CAN'T be reassigned like in regular Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Starfleet"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc4[4] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "for further help."
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc4[-4:]  # the last 3 words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "another part of the NLP pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Named Entities Recognition (**NER**)\n",
    "\n",
    "- The language model recognizes that certain words are organizational names while others are locations, and still other combinations relate to money, dates, etc. Named entities are accessible through the `ents` property of a `Doc` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starfleet | HQ | in | San | Francisco | , | and | Tokyo | costing | $ | 1.8 | Billion | a | year | \n",
      "\n",
      "Starfleet HQ - ORG - Companies, agencies, institutions, etc.\n",
      "San Francisco - GPE - Countries, cities, states\n",
      "Tokyo - GPE - Countries, cities, states\n",
      "$1.8 Billion - MONEY - Monetary values, including unit\n",
      "\n",
      "Length of string 14\n",
      "number of entities 4\n"
     ]
    }
   ],
   "source": [
    "named_ents = nlp(\"Starfleet HQ in San Francisco, and Tokyo costing $1.8 Billion a year\")\n",
    "\n",
    "for token in named_ents:\n",
    "    print(token.text, end=' | ')\n",
    "\n",
    "print('\\n')\n",
    "\n",
    "for ent in named_ents.ents:\n",
    "    print(ent.text + ' - ' + ent.label_ + ' - ' + str(spacy.explain(ent.label_)) ) \n",
    "    \n",
    "\n",
    "print('\\nLength of string', len(named_ents))\n",
    "print('number of entities', len(named_ents.ents)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that Starfleet is recognized as a Named Entity, an Organization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Noun **Chunks**\n",
    "\n",
    "- *Noun chunks* are \"base noun phrases\" – flat phrases that have a noun as their head. \n",
    "- You can think of noun chunks as a noun plus the words describing the noun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autonomous shuttles\n",
      "insurance premiums\n",
      "the Federation\n"
     ]
    }
   ],
   "source": [
    "chunky = nlp(\"Autonomous shuttles cause \\\n",
    "insurance premiums to be high for the Federation\")\n",
    "\n",
    "for chunk in chunky.noun_chunks:\n",
    "    print(chunk) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "red spacecraft\n",
      "higher insurance rates\n",
      "Federation News\n"
     ]
    }
   ],
   "source": [
    "chunky2 = nlp(\"red spacecraft have higher \\\n",
    "insurance rates debunked says Federation News\")\n",
    "\n",
    "for chunk in chunky2.noun_chunks: # noun_chunks \n",
    "    print(chunk.text) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gul Dukat\n",
      "position\n",
      "The Dominion\n",
      "Jake Sisko\n",
      "the Federation News\n"
     ]
    }
   ],
   "source": [
    "chunky3 = nlp(\"Gul Dukat fired from position with The Dominion,\\\n",
    "reports Jake Sisko from the Federation News\")\n",
    "\n",
    "for chunk in chunky3.noun_chunks:\n",
    "    print(chunk.text) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "now is the visualization part of spaCy that makes it so cool"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Built-in Visualization: **displacy**\n",
    "\n",
    "- spaCy includes a built-in visualization tool called **displaCy**. displaCy is able to detect whether you're working in a Jupyter notebook, and will return markup that can be rendered in a cell right away. When you export your notebook, the visualizations will be included as HTML.\n",
    "- https://spacy.io/usage/visualizers\n",
    "- \n",
    "- The dependency visualizer, **dep**, shows part-of-speech tags and syntactic dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy import displacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" xml:lang=\"en\" id=\"aa0c8256a6ca47d4b8e0cb14cdedd5f8-0\" class=\"displacy\" width=\"1040\" height=\"357.0\" direction=\"ltr\" style=\"max-width: none; height: 357.0px; color: #ffff66; background: #000; font-family: Arial; direction: ltr\">\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"267.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"50\">The</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"50\">DET</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"267.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"160\">Gamma</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"160\">PROPN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"267.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"270\">quadrant</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"270\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"267.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"380\">is</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"380\">AUX</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"267.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"490\">full</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"490\">ADJ</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"267.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"600\">of</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"600\">ADP</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"267.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"710\">mystery</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"710\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"267.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"820\">and</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"820\">CCONJ</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"267.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"930\">wonder</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"930\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-aa0c8256a6ca47d4b8e0cb14cdedd5f8-0-0\" stroke-width=\"2px\" d=\"M70,222.0 C70,112.0 260.0,112.0 260.0,222.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-aa0c8256a6ca47d4b8e0cb14cdedd5f8-0-0\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">det</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M70,224.0 L62,212.0 78,212.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-aa0c8256a6ca47d4b8e0cb14cdedd5f8-0-1\" stroke-width=\"2px\" d=\"M180,222.0 C180,167.0 255.0,167.0 255.0,222.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-aa0c8256a6ca47d4b8e0cb14cdedd5f8-0-1\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">compound</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M180,224.0 L172,212.0 188,212.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-aa0c8256a6ca47d4b8e0cb14cdedd5f8-0-2\" stroke-width=\"2px\" d=\"M290,222.0 C290,167.0 365.0,167.0 365.0,222.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-aa0c8256a6ca47d4b8e0cb14cdedd5f8-0-2\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M290,224.0 L282,212.0 298,212.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-aa0c8256a6ca47d4b8e0cb14cdedd5f8-0-3\" stroke-width=\"2px\" d=\"M400,222.0 C400,167.0 475.0,167.0 475.0,222.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-aa0c8256a6ca47d4b8e0cb14cdedd5f8-0-3\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">acomp</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M475.0,224.0 L483.0,212.0 467.0,212.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-aa0c8256a6ca47d4b8e0cb14cdedd5f8-0-4\" stroke-width=\"2px\" d=\"M510,222.0 C510,167.0 585.0,167.0 585.0,222.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-aa0c8256a6ca47d4b8e0cb14cdedd5f8-0-4\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">prep</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M585.0,224.0 L593.0,212.0 577.0,212.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-aa0c8256a6ca47d4b8e0cb14cdedd5f8-0-5\" stroke-width=\"2px\" d=\"M620,222.0 C620,167.0 695.0,167.0 695.0,222.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-aa0c8256a6ca47d4b8e0cb14cdedd5f8-0-5\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">pobj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M695.0,224.0 L703.0,212.0 687.0,212.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-aa0c8256a6ca47d4b8e0cb14cdedd5f8-0-6\" stroke-width=\"2px\" d=\"M400,222.0 C400,57.0 815.0,57.0 815.0,222.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-aa0c8256a6ca47d4b8e0cb14cdedd5f8-0-6\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">cc</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M815.0,224.0 L823.0,212.0 807.0,212.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-aa0c8256a6ca47d4b8e0cb14cdedd5f8-0-7\" stroke-width=\"2px\" d=\"M400,222.0 C400,2.0 930.0,2.0 930.0,222.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-aa0c8256a6ca47d4b8e0cb14cdedd5f8-0-7\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">conj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M930.0,224.0 L938.0,212.0 922.0,212.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "</svg></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "gamma = nlp(\"The Gamma quadrant is full of mystery and wonder\")\n",
    "\n",
    "# displacy.render() \n",
    "displacy.render(gamma,\n",
    "                style='dep', # parts of speech & syntax\n",
    "                jupyter=True, # True, running this code in Jupyter\n",
    "                options={\n",
    "                    'distance':110, # distance between words\n",
    "                    'color':'#ffff66', # Trek yellow\n",
    "                    'bg': '#000', # background color, black\n",
    "                    # can set type of font you want used\n",
    "#                   'font':'Montserrat'\n",
    "                    } \n",
    "               )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "another visualization for our word documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entity Recognizer Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">\n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Ferenginar stock exchange\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       " has increased profit by \n",
       "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    34.7%\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">PERCENT</span>\n",
       "</mark>\n",
       ", that's \n",
       "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    $2.12 million\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">MONEY</span>\n",
       "</mark>\n",
       "</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "doc201 = nlp(\"Ferenginar stock exchange has increased profit by 34.7%, that's $2.12 million\")\n",
    "\n",
    "displacy.render(\n",
    "    doc201,\n",
    "    style='ent', # entity\n",
    "    jupyter=True,\n",
    "    options={\n",
    "        'color':'#000',\n",
    "        'bg': '#ff3399'\n",
    "        } \n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "incase you have a hard time seeing the print out on a dark Notebook\n",
    "\n",
    "\n",
    "<font color='#ffff66'> Ferenginar stock exchange ORG </font>\n",
    "\n",
    "has increased profit by <font color='#66ff99'>34.7% PERCENT</font> , \n",
    "\n",
    "\n",
    "that's <font color='#66ff99'> $2.12 million MONEY </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "more pipeline processing "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemmatization\n",
    "- lemmatization looks beyond word reduction, and considers a language's full vocabulary to apply a morphological analysis to words.\n",
    "- The lemma of 'was' is 'be' and the lemma of 'mice' is 'mouse'.\n",
    "- Further, the lemma of 'meeting' might be 'meet' or 'meeting' depending on its use in a sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The \t DET \t 7425985699627899538 \t the\n",
      "Chief \t PROPN \t 9615737558093935814 \t Chief\n",
      "of \t ADP \t 886050111519832510 \t of\n",
      "Engineering \t PROPN \t 17806784801972435463 \t Engineering\n",
      "says \t VERB \t 8685289367999165211 \t say\n",
      "the \t DET \t 7425985699627899538 \t the\n",
      "warpcore \t NOUN \t 15559318848824790373 \t warpcore\n",
      "engine \t NOUN \t 10857443142397967886 \t engine\n",
      "needs \t VERB \t 478886015463313967 \t need\n",
      "an \t DET \t 15099054000809333061 \t an\n",
      "update \t NOUN \t 1936357517718432020 \t update\n"
     ]
    }
   ],
   "source": [
    "lemmy = nlp(\"The Chief of Engineering says the warpcore engine needs an update\")\n",
    "\n",
    "for token in lemmy:\n",
    "     print(token.text, '\\t', token.pos_, '\\t', token.lemma, '\\t', token.lemma_) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "spaCy has not seen warpcore before and labels it noun.\n",
    "\n",
    "\n",
    "Note: the long numbers are spaCy's lemma reference numbers that identify how common the word is used in the English language sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "okay, let's repeat a word that has a stem and see what the lemmatization labels it as\n",
    "- example: run (stem), running, runs, run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the \t DET \t# 7425985699627899538 \t the\n",
      "Chief \t PROPN \t# 9615737558093935814 \t Chief\n",
      "of \t ADP \t# 886050111519832510 \t of\n",
      "Engineering \t PROPN \t# 17806784801972435463 \t Engineering\n",
      "runs \t VERB \t# 12767647472892411841 \t run\n",
      "around \t ADV \t# 3194226484742107227 \t around\n",
      ", \t PUNCT \t# 2593208677638477497 \t ,\n",
      "always \t ADV \t# 17471638809377599778 \t always\n",
      "running \t VERB \t# 12767647472892411841 \t run\n",
      "software \t NOUN \t# 8212201967714533330 \t software\n",
      "to \t PART \t# 3791531372978436496 \t to\n",
      "see \t VERB \t# 11925638236994514241 \t see\n",
      "if \t SCONJ \t# 12446819118446800910 \t if\n",
      "we \t PRON \t# 561228191312463089 \t -PRON-\n",
      "have \t AUX \t# 14692702688101715474 \t have\n",
      "any \t DET \t# 13148361048351484388 \t any\n",
      "errors \t NOUN \t# 5141748423479617815 \t error\n",
      "in \t ADP \t# 3002984154512732771 \t in\n",
      "the \t DET \t# 7425985699627899538 \t the\n",
      "running \t NOUN \t# 12212083579121184944 \t running\n",
      "program \t NOUN \t# 17812688126189747487 \t program\n",
      ", \t PUNCT \t# 2593208677638477497 \t ,\n",
      "it \t PRON \t# 561228191312463089 \t -PRON-\n",
      "'s \t AUX \t# 10382539506755952630 \t be\n",
      "a \t DET \t# 11901859001352538922 \t a\n",
      "race \t NOUN \t# 8048469955494714898 \t race\n"
     ]
    }
   ],
   "source": [
    "lemmy2 = nlp(\"the Chief of Engineering runs around, \\\n",
    "always running software to see if we have any errors in the running program, it's a race\")\n",
    "\n",
    "for token in lemmy2:\n",
    "    print(token.text, '\\t', token.pos_, \n",
    "          '\\t#', token.lemma, '\\t', token.lemma_) "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "notice that the words that stem from 'run' share a similar hash value\n",
    "\n",
    "    runs \t VERB \t# 12767647472892411841 \t run\n",
    " running \t VERB \t# 12767647472892411841 \t run\n",
    " running \t NOUN \t# 12212083579121184944 \t running\n",
    "    race \t NOUN \t# 8048469955494714898 \t race"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display the lemmas nicely\n",
    "def show_lemmas(text):\n",
    "    for token in text:\n",
    "        # using f-string formatting, numbers determine spacing\n",
    "        print(f'{token.text:{12}} {token.pos_:{6}} {token.lemma:<{22}} {token.lemma_}') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the          DET    7425985699627899538    the\n",
      "Chief        PROPN  9615737558093935814    Chief\n",
      "of           ADP    886050111519832510     of\n",
      "Engineering  PROPN  17806784801972435463   Engineering\n",
      "runs         VERB   12767647472892411841   run\n",
      "around       ADV    3194226484742107227    around\n",
      ",            PUNCT  2593208677638477497    ,\n",
      "always       ADV    17471638809377599778   always\n",
      "running      VERB   12767647472892411841   run\n",
      "software     NOUN   8212201967714533330    software\n",
      "to           PART   3791531372978436496    to\n",
      "see          VERB   11925638236994514241   see\n",
      "if           SCONJ  12446819118446800910   if\n",
      "we           PRON   561228191312463089     -PRON-\n",
      "have         AUX    14692702688101715474   have\n",
      "any          DET    13148361048351484388   any\n",
      "errors       NOUN   5141748423479617815    error\n",
      "in           ADP    3002984154512732771    in\n",
      "the          DET    7425985699627899538    the\n",
      "running      NOUN   12212083579121184944   running\n",
      "program      NOUN   17812688126189747487   program\n",
      ",            PUNCT  2593208677638477497    ,\n",
      "it           PRON   561228191312463089     -PRON-\n",
      "'s           AUX    10382539506755952630   be\n",
      "a            DET    11901859001352538922   a\n",
      "race         NOUN   8048469955494714898    race\n"
     ]
    }
   ],
   "source": [
    "show_lemmas(lemmy2) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "as fun as that is, for further analysis requires to remove certain words that do not help in finding patterns or specific word associations, these are called *stop words*. Stop words is a huge python set of words like \"umm\",\"been\", etc. and can be modified to add new words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **STOP WORDS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of stop_words: 326\n"
     ]
    }
   ],
   "source": [
    "# Print the set of spaCy's default stop words \n",
    "# (remember that sets are unordered):\n",
    "\n",
    "#-- uncomment to see the words\n",
    "# print(nlp.Defaults.stop_words) \n",
    "\n",
    "print('number of stop_words:', len(nlp.Defaults.stop_words) ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check if a word is part of this set\n",
    "nlp.vocab['is'].is_stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.vocab['space'].is_stop "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add a **stop_word**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "327"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 'btw' = by the way\n",
    "# Add the word to the set of stop words. Use lowercase!\n",
    "nlp.Defaults.stop_words.add('btw')\n",
    "\n",
    "# Set the stop_word tag on the lexeme\n",
    "nlp.vocab['btw'].is_stop = True\n",
    "\n",
    "# 326 stop_words before\n",
    "len(nlp.Defaults.stop_words) # added it to the set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove a **stop_word**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word 'beyond' to be removed for example\n",
    "# Remove the word from the set of stop words\n",
    "nlp.Defaults.stop_words.remove('beyond')\n",
    "\n",
    "# Remove the stop_word tag from the lexeme\n",
    "nlp.vocab['beyond'].is_stop = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "326"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(nlp.Defaults.stop_words) # dropped the word from the set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Phrases: Vocab & Matching\n",
    "\n",
    "- spaCy offers a rule-matching tool called **Matcher** that allows you to build a library of token patterns, then match those patterns against a Doc object to return a list of found matches. \n",
    "- You can match on any part of the token including text and annotations, and you can add multiple patterns to the same matcher.\n",
    "\n",
    "rule-based Token Matcher attributes\n",
    "<table><tr><th>Attribute</th><th>Description</th></tr>\n",
    "\n",
    "<tr ><td><span >`ORTH`</span></td><td>The exact verbatim text of a token</td></tr>\n",
    "<tr ><td><span >`LOWER`</span></td><td>The lowercase form of the token text</td></tr>\n",
    "<tr ><td><span >`LENGTH`</span></td><td>The length of the token text</td></tr>\n",
    "<tr ><td><span >`IS_ALPHA`, `IS_ASCII`, `IS_DIGIT`</span></td><td>Token text consists of alphanumeric characters, ASCII characters, digits</td></tr>\n",
    "<tr ><td><span >`IS_LOWER`, `IS_UPPER`, `IS_TITLE`</span></td><td>Token text is in lowercase, uppercase, titlecase</td></tr>\n",
    "<tr ><td><span >`IS_PUNCT`, `IS_SPACE`, `IS_STOP`</span></td><td>Token is punctuation, whitespace, stop word</td></tr>\n",
    "<tr ><td><span >`LIKE_NUM`, `LIKE_URL`, `LIKE_EMAIL`</span></td><td>Token text resembles a number, URL, email</td></tr>\n",
    "<tr ><td><span >`POS`, `TAG`, `DEP`, `LEMMA`, `SHAPE`</span></td><td>The token's simple and extended part-of-speech tag, dependency label, lemma, shape</td></tr>\n",
    "<tr ><td><span >`ENT_TYPE`</span></td><td>The token's entity label</td></tr>\n",
    "\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the Matcher library\n",
    "from spacy.matcher import Matcher\n",
    "matcher = Matcher(nlp.vocab) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "using a list that holds a dictionary, the dictionary holds the attribute and the word to find a pattern match for. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a pattern to match\n",
    "pattern1 = [ {'LOWER': 'warpcore'} ]\n",
    "pattern2 = [ {'LOWER': 'warp'}, {'LOWER': 'core'}, ] # warp core\n",
    "pattern3 = [ {'LOWER': 'warp'}, {'IS_PUNCT': True}, {'LOWER': 'core'}, ] # warp-core\n",
    "\n",
    "matcher.add('WarpCore', None, pattern1, pattern2, pattern3) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- pattern1 looks for 'warpcore'\n",
    "- pattern2 looks for 'warp core'\n",
    "- pattern3 looks for 'warp-core'\n",
    "- callbacks are set to None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply **matcher** to a doc object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(10517466410332998210, 3, 4), (10517466410332998210, 8, 11), (10517466410332998210, 20, 22)]\n"
     ]
    }
   ],
   "source": [
    "matchy = nlp(\"There is a warpcore breach due to the warp-core not being properly installed \\\n",
    "like the manufacturer of the warp core states in the manual\")\n",
    "\n",
    "found_matches = matcher(matchy)\n",
    "print(found_matches) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WarpCore 3 4 moving\n",
      "WarpCore 8 11 \n",
      "WarpCore 20 22 \n"
     ]
    }
   ],
   "source": [
    "for match_id, start, end in found_matches:\n",
    "    string_id = nlp.vocab.strings[match_id]  # get string representation\n",
    "    span = doc[start:end]                    # get the matched span\n",
    "    print( string_id, start, end, span.text) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Patterns optional token rules\n",
    "\n",
    "The following quantifiers can be passed to the `'OP'` key:\n",
    "<table><tr><th>OP</th><th>Description</th></tr>\n",
    "\n",
    "<tr ><td><span >\\!</span></td><td>Negate the pattern, by requiring it to match exactly 0 times</td></tr>\n",
    "<tr ><td><span >?</span></td><td>Make the pattern optional, by allowing it to match 0 or 1 times</td></tr>\n",
    "<tr ><td><span >\\+</span></td><td>Require the pattern to match 1 or more times</td></tr>\n",
    "<tr ><td><span >\\*</span></td><td>Allow the pattern to match zero or more times</td></tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a pattern to match\n",
    "pattern1 = [ {'LOWER': 'warpcore'} ] \n",
    "pattern3 = [ {'LOWER': 'warp'}, {'IS_PUNCT': True, 'OP':'*'}, {'LOWER': 'core'}, ] # warp-core\n",
    "\n",
    "# Remove the old patterns to avoid duplication:\n",
    "# matcher.remove('warpcore') \n",
    "\n",
    "matcher.add('WarpCore', None, pattern1, pattern3) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(10517466410332998210, 7, 8), (10517466410332998210, 13, 16)]\n"
     ]
    }
   ],
   "source": [
    "matchy2 = nlp(\"it is important to have a maintained warpcore, unless you want a warp--core\")\n",
    "\n",
    "found_matches = matcher(matchy2) \n",
    "print(found_matches) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "warpcore\n",
      "warp--core\n"
     ]
    }
   ],
   "source": [
    "print(matchy2[7:8] )\n",
    "print(matchy2[13:16]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "now we move on to phrase matching from a document "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phrase Matcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the PhraseMatcher library\n",
    "from spacy.matcher import PhraseMatcher\n",
    "matcher = PhraseMatcher(nlp.vocab) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://en.wikipedia.org/wiki/Starfleet\n",
    "\n",
    "# open the saved file\n",
    "with open(\"starfleet.txt\", encoding=\"utf8\") as f:\n",
    "    wiki_doc = nlp(f.read()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  phrases list for matching\n",
    "phrase_list = ['Starfleet','engineer','Trek','Academy','Medical']\n",
    "\n",
    "# Next, convert each phrase to a Doc object:\n",
    "#  list comprehension\n",
    "phrase_patterns = [nlp(text) for text in phrase_list] \n",
    "\n",
    "# Pass each Doc object into matcher (note the use of the asterisk!):\n",
    "matcher.add('federation', None, *phrase_patterns)\n",
    "\n",
    "# Build a list of matches:\n",
    "found_matches = matcher(wiki_doc) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Starfleet, engineer, Trek, Academy, Medical]"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "phrase_patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "43"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(found_matches) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "federation 0 1 Starfleet\n",
      "federation 8 9 Trek\n",
      "federation 17 18 Starfleet\n",
      "federation 56 57 Starfleet\n",
      "federation 75 76 Trek\n",
      "federation 84 85 Starfleet\n",
      "federation 112 113 Starfleet\n",
      "federation 119 120 Starfleet\n",
      "federation 166 167 Starfleet\n",
      "federation 178 179 Starfleet\n",
      "federation 179 180 Academy\n",
      "federation 187 188 Trek\n",
      "federation 193 194 Starfleet\n",
      "federation 194 195 Academy\n",
      "federation 219 220 Starfleet\n",
      "federation 231 232 Starfleet\n",
      "federation 241 242 Starfleet\n",
      "federation 246 247 Starfleet\n",
      "federation 280 281 Trek\n",
      "federation 287 288 Trek\n",
      "federation 331 332 Trek\n",
      "federation 340 341 Starfleet\n",
      "federation 351 352 Starfleet\n",
      "federation 377 378 Starfleet\n",
      "federation 384 385 Starfleet\n",
      "federation 414 415 Trek\n",
      "federation 437 438 Trek\n",
      "federation 460 461 Trek\n",
      "federation 479 480 Starfleet\n",
      "federation 503 504 Trek\n",
      "federation 515 516 Starfleet\n",
      "federation 531 532 Starfleet\n",
      "federation 532 533 Medical\n",
      "federation 538 539 Starfleet\n",
      "federation 551 552 Trek\n",
      "federation 579 580 Starfleet\n",
      "federation 580 581 Medical\n",
      "federation 583 584 Starfleet\n",
      "federation 590 591 Starfleet\n",
      "federation 598 599 Trek\n",
      "federation 605 606 Trek\n",
      "federation 616 617 Starfleet\n",
      "federation 623 624 Trek\n"
     ]
    }
   ],
   "source": [
    "# to print out where the word 'federation' appears in the document \n",
    "# it also give the word next to the pattern word\n",
    "\n",
    "for match_id, start, end in found_matches:\n",
    "    string_id = nlp.vocab.strings[match_id]  # get string representation\n",
    "    span = wiki_doc[start:end]                    # get the matched span\n",
    "#     print(match_id, string_id, start, end, span.text) \n",
    "    print( string_id, start, end, span.text) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "federation 0 1 \n",
      "federation 8 9 in the Star Trek media franchise.\n",
      "federation 17 18 fictional universe, Starfleet is a uniformed\n",
      "federation 56 57 diplomacy (although Starfleet predates the Federation\n",
      "federation 75 76 television series Star Trek: Enterprise)\n",
      "federation 84 85 the majority of Starfleet's members are\n",
      "federation 112 113 's protagonists are Starfleet commissioned officers.\n",
      "federation 119 120 \n",
      "\n",
      "Mission: Starfleet has been shown\n",
      "federation 166 167 The flagship of Starfleet is often considered\n",
      "federation 178 179 Enterprise.\n",
      "\n",
      "Starfleet Academy: As\n",
      "federation 179 180 .\n",
      "\n",
      "Starfleet Academy: As early\n",
      "federation 187 188 the original Star Trek, characters refer\n",
      "federation 193 194 refer to attending Starfleet Academy. Later\n",
      "federation 194 195 to attending Starfleet Academy. Later series\n",
      "federation 219 220 is located near Starfleet Headquarters in what\n",
      "federation 231 232 California.\n",
      "\n",
      "Starfleet Command: is\n",
      "federation 241 242 command center of Starfleet. The term\n",
      "federation 246 247 The term \"Starfleet Command\" is\n",
      "federation 280 281 , in Star Trek: The Motion\n",
      "federation 287 288 Picture and Star Trek IV: The\n",
      "federation 331 332 Throughout the Star Trek franchise, the\n",
      "federation 340 341 ' isolation from Starfleet Command compels them\n",
      "federation 351 352 upon decisions without Starfleet Command's orders\n",
      "federation 377 378 .\n",
      "\n",
      "The Starfleet Engineering Corps (\n",
      "federation 384 385 also called the Starfleet Corps of Engineers\n",
      "federation 414 415 asteroid in Star Trek II: The\n",
      "federation 437 438 in the Star Trek: Voyager episode\n",
      "federation 460 461 in the Star Trek: Deep Space\n",
      "federation 479 480 these successes, Starfleet engineers gained a\n",
      "federation 503 504 in the Star Trek: DS9 episode\n",
      "federation 515 516 \" notes, Starfleet engineers are reputed\n",
      "federation 531 532 .\"\n",
      "\n",
      "Starfleet Medical is the\n",
      "federation 532 533 \"\n",
      "\n",
      "Starfleet Medical is the medical\n",
      "federation 538 539 medical branch of Starfleet: Gates McFadden\n",
      "federation 551 552 , left Star Trek: The Next\n",
      "federation 579 580 been assigned to Starfleet Medical.\n",
      "\n",
      "\n",
      "federation 580 581 assigned to Starfleet Medical.\n",
      "\n",
      "Starfleet\n",
      "federation 583 584 Medical.\n",
      "\n",
      "Starfleet Security: is\n",
      "federation 590 591 an agency of Starfleet referred to in\n",
      "federation 598 599 episodes of Star Trek: The Next\n",
      "federation 605 606 Generation and Star Trek: Deep Space\n",
      "federation 616 617 a branch of Starfleet first introduced in\n",
      "federation 623 624 the original Star Trek. Main characters\n"
     ]
    }
   ],
   "source": [
    "# the context of the word in the document\n",
    "for match_id, start, end in found_matches:\n",
    "    string_id = nlp.vocab.strings[match_id]  # get string representation\n",
    "    span = wiki_doc[start-3:end+3]                    # get the matched span\n",
    "    print(string_id, start, end, span.text) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starfleet | Trek | Starfleet\n"
     ]
    }
   ],
   "source": [
    "# the phrase matcher of the document span\n",
    "# the bigger the slice the more context of the phrase\n",
    "x = wiki_doc[0:1] \n",
    "v = wiki_doc[8:9] \n",
    "m = wiki_doc[17:18]\n",
    "print(\"{} | {} | {}\".format(x,v,m)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "as an officer training facility with a four-year educational program. The main campus is located near Starfleet Headquarters in what is now Fort Baker, California.\n",
       "\n",
       "Starfleet Command: is the headquarters/command center of Starfleet. The term \"Starfleet Command\" is"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wiki_doc[200:250] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 12\n"
     ]
    }
   ],
   "source": [
    "# Build a list of sentences\n",
    "sents = [sent for sent in wiki_doc.sents]\n",
    "\n",
    "# In the next section we'll see that sentences contain start and end token values:\n",
    "print(sents[0].start, sents[0].end) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "While the majority of Starfleet's members are human and it is headquartered on Earth, hundreds of other species are also represented."
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sents[2] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "The Wrath of Khan, the design of the Yellowstone-class Runabout in the alternate timeline in the Star Trek: Voyager episode \"Non Sequitur\", and devising a defense against the Breen energy-dampening weapon in the Star Trek:"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sents[17]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
